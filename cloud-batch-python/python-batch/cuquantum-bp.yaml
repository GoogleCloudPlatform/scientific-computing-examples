# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

### IMPORTANT NOTES
# (1) Availability of A2 and Cloud Batch regions are limited to a small number
#  of good region / zone combinations: (quota in each is 10 GPUs)
# - us-central1 / us-central1-c
# - us-east1 / us-east1-b
# (2) the project must have the following properties
# - default Compute Engine account must have high permisisons (e.g. Editor)
# - should probably already have the following APIs enabled
#   - batch, compute, serviceusage, storage

---
  blueprint_name: batch-quantum-ai
  
  # Please review https://cloud.google.com/compute/docs/regions-zones
  # for availability of A2 machine types
  vars:
    project_id: cq-test-12
    deployment_name: quantum-ai
    region: us-east1
    zone: us-east1-b
    new_image_family: nvidia-batch
    machine_type: a2-highgpu-1g
    network_name: batch-demo-network
    subnetwork_name: batch-demo-subnetwork
    enable_public_ips: false
  
  deployment_groups:
  - group: setup
    modules:
    - id: network0
      source: modules/network/vpc
  
  - group: packer
    modules:
    - id: custom-image
      source: modules/packer/custom-image
      kind: packer
      settings:
        disk_size: 50
        image_family: $(vars.new_image_family)
        source_image_family: ubuntu-2004-lts
        startup_script: |
          #!/bin/bash
          # Install GPU drivers via supported Google Cloud script
          set -e -o pipefail
          curl -O https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py
          python3 install_gpu_driver.py
          # Install NVIDIA Container Toolkit
          distribution=\$(. /etc/os-release;echo $ID$VERSION_ID) \
            && curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
            && curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
            sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
            tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
          apt-get update
          apt-get install -y nvidia-docker2
          systemctl restart docker
          # Install gcsfuse
          RELEASE=\$(lsb_release -c -s)
          export GCSFUSE_REPO="gcsfuse-${RELEASE}"
          echo "deb http://packages.cloud.google.com/apt $GCSFUSE_REPO main" | tee /etc/apt/sources.list.d/gcsfuse.list
          curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
          apt-get update
          apt-get -y install gcsfuse
  
  - group: batch
    modules:
    - id: network1
      source: modules/network/pre-existing-vpc
  
    - id: bucket
      source: github.com/terraform-google-modules/terraform-google-cloud-storage?ref=v3.4.0
      settings:
        location: $(vars.region)
        prefix: batch
        names:
        - $(vars.project_id)
  
    - id: bucket_fs
      source: modules/file-system/pre-existing-network-storage
      settings:
        remote_mount: $(bucket.name)
        local_mount: /scratch
        fs_type: gcsfuse
        mount_options: defaults,_netdev,implicit_dirs,allow_other
  
    - id: batch-job
      source: modules/scheduler/batch-job-template
      use:
      - network1
      - bucket_fs
      settings:
        runnable: | 
          #!/bin/bash 
          docker run --gpus all \
            -v /scratch:/scratch \
            --entrypoint="/bin/bash" \
            nvcr.io/nvidia/cuquantum-appliance:22.07-cirq \
            -c  "python3 /workspace/examples/ghz.py  --nqubits 32 --nsamples 3000 --ngpus 1 >> /scratch/demo3.txt" 
        image:
          family: $(vars.new_image_family)
          project: $(vars.project_id)
      outputs:
        - instructions    

