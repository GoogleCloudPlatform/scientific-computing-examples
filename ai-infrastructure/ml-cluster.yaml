---
blueprint_name: ml-cluster

vars:
  project_id: PROJECT_NAME ## Set your project id
  deployment_name: llama2-hpc
  region: REGION ## Set your region
  zone: ZONE ## Set your zone
  network_name: llama-network
  subnetwork_name: llama2-subnet
  new_image_family: llama2-slurm
  disk_size_gb: 200
  bucket_model: BUCKET_NAME ## Set your bucket name

deployment_groups:
- group: setup
  modules:
  - id: network1
    source: modules/network/vpc

  - id: homefs
    source: modules/file-system/filestore
    use:
    - network1
    settings:
      local_mount: /home

  - id: bucket_create
    source: community/modules/file-system/cloud-storage-bucket
    settings:
      name_prefix: $(vars.bucket_model)
      use_deployment_name_in_bucket_name: false

  - id: bucket_mount
    source: modules/file-system/pre-existing-network-storage
    settings:
      remote_mount: $(vars.bucket_model)
      local_mount: /bucket
      fs_type: gcsfuse
      mount_options: defaults,_netdev,implicit_dirs,allow_other,dir_mode=0777,file_mode=766

  - id: conda_env
    # environment file for conda
    source: modules/scripts/startup-script
    settings:
      runners:
      - type: data
        destination: /tmp/junk
        content: |
          name: llama2
          channels:
            - conda-forge
            - nvidia
            - nvidia/label/cuda-12.2.0
          dependencies:
            - appdirs
            - loralib
            - black
            - black-jupyter
            - py7zr
            - scipy
            - optimum
            - datasets
            - accelerate
            - peft
            - fairscale
            - fire
            - sentencepiece
            - transformers
            - huggingface_hub
            - git
            - pip
            - pip:
              - bitsandbytes
              - nvidia-cudnn-cu12
              - nvidia-nccl-cu12
              - trl
              - torch
              - torchaudio 
              - torchvision

  - id: script
    # configure conda environment for llama
    source: modules/scripts/startup-script
    settings:
      runners:
      - type: shell
        destination: install-ml-libraries.sh
        content: |
          #!/bin/bash
          # this script is designed to execute on Slurm images published by SchedMD that:
          # - are based on Debian 11 distribution of Linux
          # - have NVIDIA Drivers v530 pre-installed
          # - have CUDA Toolkit 12.1 pre-installed.

          set -x
          set -e -o pipefail
         
          tee /etc/yum.repos.d/google-fast-socket.repo << EOM
          [google-fast-socket]
          name=Fast Socket Transport for NCCL
          baseurl=https://packages.cloud.google.com/yum/repos/google-fast-socket
          enabled=1
          gpgcheck=0
          repo_gpgcheck=0
          gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
                https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
          EOM
          yum -y install google-fast-socket

          CONDA_BASE=/opt/conda

          if [ -d $CONDA_BASE ]; then
                  exit 0
          fi

          DL_DIR=\$(mktemp -d)
          cd $DL_DIR

          wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
          bash miniconda.sh -b -u -p ${CONDA_BASE}
          ${CONDA_BASE}/bin/conda init bash

          cd -
          rm -rf $DL_DIR
          unset DL_DIR

          tee /tmp/llama2_env.yml << EOLL
          name: llama2
          channels:
            - conda-forge
            - nvidia
            - nvidia/label/cuda-12.2.0
          dependencies:
            - appdirs
            - loralib
            - black
            - black-jupyter
            - py7zr
            - scipy
            - optimum
            - datasets
            - accelerate
            - peft
            - fairscale
            - fire
            - sentencepiece
            - transformers
            - huggingface_hub
            - git
            - pip
            - pip:
              - bitsandbytes
              - nvidia-cudnn-cu12
              - nvidia-nccl-cu12
              - trl
              - torch
              - torchaudio 
              - torchvision
          EOLL

          source $CONDA_BASE/bin/activate base
          conda env create -n llama2 --file /tmp/llama2_env.yml

- group: packer
  modules:
  - id: custom-image
    source: modules/packer/custom-image
    kind: packer
    use:
    - network1
    - script
    settings:
      source_image_project_id: [schedmd-slurm-public]
      source_image_family: slurm-gcp-5-9-hpc-centos-7
      disk_size: $(vars.disk_size_gb)
      image_family: $(vars.new_image_family)
      machine_type: c2-standard-8 # building this image does not require a GPU-enabled VM
      state_timeout: 30m

- group: cluster
  modules:
  - id: ssd-startup
    source: modules/scripts/startup-script
    settings:
      runners:
      - type: shell
        destination: "/tmp/mount_ssd.sh"
        content: |
          #!/bin/bash
          set -ex
          export LOG_FILE=/tmp/ssd-setup.log
          export DST_MNT="/scratch"
          if [ -d $DST_MNT ]; then
              echo "DST_MNT already exists. Canceling." >> $LOG_FILE
              exit 0
          fi
          apt -y install mdadm
          lsblk >> $LOG_FILE
          export DEVICES=`lsblk -d -n -oNAME,RO | grep 'nvme.*0$' | awk {'print "/dev/" $1'}`
          mdadm --create /dev/md0 --level=0 --raid-devices=8 $DEVICES
          mkfs.ext4 -F /dev/md0
          mkdir -p $DST_MNT
          mount /dev/md0 $DST_MNT
          chmod a+w $DST_MNT
          echo UUID=`blkid -s UUID -o value /dev/md0` $DST_MNT ext4 discard,defaults,nofail 0 2 | tee -a /etc/fstab
          cat /etc/fstab >> $LOG_FILE
          echo "DONE" >> $LOG_FILE

  - id: a2_node_group
    source: community/modules/compute/schedmd-slurm-gcp-v5-node-group
    settings:
      node_count_dynamic_max: 2
      bandwidth_tier: gvnic_enabled
      disable_public_ips: false
      enable_smt: true
      machine_type: a2-ultragpu-8g
      disk_type: pd-ssd
      disk_size_gb: $(vars.disk_size_gb)
      on_host_maintenance: TERMINATE
      instance_image:
        family: $(vars.new_image_family)
        project: $(vars.project_id)

  - id: a2_partition
    source: community/modules/compute/schedmd-slurm-gcp-v5-partition
    use:
    - ssd-startup
    - a2_node_group
    - homefs
    - bucket_mount
    - network1
    settings:
      zone: $(vars.zone)
      partition_name: a100
      enable_placement: true

  - id: g2_node_group
    source: community/modules/compute/schedmd-slurm-gcp-v5-node-group
    settings:
      node_count_dynamic_max: 5
      bandwidth_tier: gvnic_enabled
      disable_public_ips: false
      enable_smt: true
      machine_type: g2-standard-96
      disk_type: pd-ssd
      disk_size_gb: $(vars.disk_size_gb)
      on_host_maintenance: TERMINATE
      instance_image:
        family: $(vars.new_image_family)
        project: $(vars.project_id)

  - id: g2_partition
    source: community/modules/compute/schedmd-slurm-gcp-v5-partition
    use:
    - g2_node_group
    - homefs
    - bucket_mount
    - network1
    settings:
      partition_name: l4
      enable_placement: false # enable compact placement policy
      partition_startup_scripts_timeout: 1200

  - id: slurm_controller
    source: community/modules/scheduler/schedmd-slurm-gcp-v5-controller
    use:
    - network1
    - a2_partition
    - homefs
    - bucket_mount
    settings:
      disable_controller_public_ips: false
      instance_image:
        family: $(vars.new_image_family)
        project: $(vars.project_id)

  - id: slurm_login
    source: community/modules/scheduler/schedmd-slurm-gcp-v5-login
    use:
    - network1
    - slurm_controller
    settings:
      instance_image:
        family: $(vars.new_image_family)
        project: $(vars.project_id)